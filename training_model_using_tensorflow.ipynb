{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40b2db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371/371 [==============================] - 7s 17ms/step - loss: 0.0978 - accuracy: 0.9694\n",
      "371/371 [==============================] - 4s 10ms/step - loss: 0.0915 - accuracy: 0.9703\n",
      "371/371 [==============================] - 4s 10ms/step - loss: 0.0937 - accuracy: 0.9722\n",
      "371/371 [==============================] - 5s 14ms/step - loss: 0.0995 - accuracy: 0.9697\n",
      "371/371 [==============================] - 5s 13ms/step - loss: 0.0899 - accuracy: 0.9712\n",
      "Average Accuracy: 0.97\n",
      "INFO:tensorflow:Assets written to: trained_model_GPT.joblib\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model_GPT.joblib\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data (replace the filename with your actual dataset)\n",
    "data = pd.read_csv('feature-updated-dataset.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X_textual = data['url']  # Textual data (URLs)\n",
    "# X_numerical = data.drop(columns=['url', 'label'])  # Numerical data excluding URLs and labels\n",
    "X_numerical = data.iloc[:, 2:12]  # Extract first 10 numerical features (columns 2 to 11)\n",
    "\n",
    "y = data['label']\n",
    "\n",
    "# Define the number of folds (k) for cross-validation\n",
    "k = 5\n",
    "\n",
    "# Initialize the Tokenizer for textual data\n",
    "max_words = 5000  # The maximum number of words to keep in the vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_textual)\n",
    "\n",
    "# Convert textual data to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(X_textual)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "X_textual = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Define the number of classes (malicious and not malicious)\n",
    "num_classes = 2\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "# Initialize arrays to store cross-validation results\n",
    "accuracy_scores = []\n",
    "\n",
    "# Create the k-fold cross-validation object\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X_textual):\n",
    "    X_textual_train, X_textual_test = X_textual[train_index], X_textual[test_index]\n",
    "    X_numerical_train, X_numerical_test = X_numerical[train_index], X_numerical[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Create the LSTM model for textual data\n",
    "    input_textual = Input(shape=(max_sequence_length,))\n",
    "    x = Embedding(input_dim=max_words, output_dim=32, input_length=max_sequence_length)(input_textual)\n",
    "    x = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "    textual_output = Dense(32)(x)\n",
    "\n",
    "    # Create the Dense model for numerical data\n",
    "    input_numerical = Input(shape=(X_numerical_train.shape[1],))\n",
    "    numerical_output = Dense(32)(input_numerical)\n",
    "\n",
    "    # Concatenate the textual and numerical outputs\n",
    "    concatenated = concatenate([textual_output, numerical_output])\n",
    "    output = Dense(num_classes, activation='softmax')(concatenated)\n",
    "\n",
    "    # Create the combined model\n",
    "    model = Model(inputs=[input_textual, input_numerical], outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit([X_textual_train, X_numerical_train], y_train, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    _, accuracy = model.evaluate([X_textual_test, X_numerical_test], y_test)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_model_GPT.joblib')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c1dcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_model_GPT2.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save('trained_model_GPT.joblib')\n",
    "# print(\"Model saved successfully!\")\n",
    "\n",
    "# Save the fitted scaler to a file\n",
    "# joblib.dump(scaler, 'fitted_scaler.pkl')\n",
    "\n",
    "import joblib\n",
    "joblib.dump(model, 'trained_model_GPT2.joblib', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee868dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahab\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 232ms/step\n",
      "The URL is Malicious.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import joblib\n",
    "\n",
    "# Function to preprocess textual data and numerical features\n",
    "def preprocess_data(url, numerical_features, tokenizer, fitted_scaler):\n",
    "    # Preprocess textual data (URL)\n",
    "    sequences = tokenizer.texts_to_sequences([url])\n",
    "    max_sequence_length = 86 # Insert the value used during training\n",
    "    X_textual = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Preprocess numerical data\n",
    "    numerical_features = fitted_scaler.transform([numerical_features])\n",
    "\n",
    "    return X_textual, numerical_features\n",
    "\n",
    "# Function to check if the URL is malicious\n",
    "def is_malicious(url, numerical_features, tokenizer, fitted_scaler, model):\n",
    "    X_textual, X_numerical = preprocess_data(url, numerical_features, tokenizer, fitted_scaler)\n",
    "    y_pred = model.predict([X_textual, X_numerical])\n",
    "    is_malicious = np.argmax(y_pred, axis=1)[0]  # Get the class prediction (0 or 1)\n",
    "\n",
    "    # Assuming class 0 represents non-malicious and class 1 represents malicious\n",
    "    if is_malicious == 0:\n",
    "        return \"Non-Malicious\"\n",
    "    else:\n",
    "        return \"Malicious\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model\n",
    "#     model = load_model('trained_model_GPT3.joblib')\n",
    "    model_filename = 'trained_model_GPT3.joblib'\n",
    "    model = joblib.load(model_filename)\n",
    "\n",
    "    # Load the Tokenizer and fitted scaler used during training\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts([''])  # Empty list to initialize the tokenizer (URLs will be added later)\n",
    "    fitted_scaler = joblib.load('fitted_scaler.pkl')\n",
    "\n",
    "    # Example usage\n",
    "    url_input = \"https://yahoo.com\"  # Replace with the URL to be checked\n",
    "#     url_input = \"http://162.144.84.82/~tdsecureupdate/Tdbank.com/step2.php\"\n",
    "    numerical_features_input = [0.1, 0.5, 0.3, 0.2, 0.7, 0.4, 0.9, 0.6, 0.8, 0.2]\n",
    "\n",
    "    # Check if the URL is malicious\n",
    "    result = is_malicious(url_input, numerical_features_input, tokenizer, fitted_scaler, model)\n",
    "    print(f\"The URL is {result}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
