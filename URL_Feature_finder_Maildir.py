# Python CentOS find URL in body
import os
import re
import csv
from nltk import word_tokenize
from nltk.tokenize import word_tokenize, sent_tokenize
from math import log2
from collections import Counter
import string



# Directory selection
# home_path = '~'               # Linux
home_path = 'd:/files/home/'    # Windows


def count_html_tags(content):
    # Regular expression pattern to match HTML tags
    html_tag_pattern = r"<[^>]+>"

    # Find all HTML tags in the content
    html_tags = re.findall(html_tag_pattern, content)

    # Return the count of HTML tags
    return len(html_tags)


def calculate_html_length(content):
    # Calculate the length of HTML content (excluding tags)
    html_length = len(re.sub(r"<[^>]+>", "", content))
    return html_length


def calculate_number_of_tokens(content):
    # Tokenize the content and count the number of tokens
    tokens = word_tokenize(content)
    return len(tokens)


def calculate_number_of_sentences(content):
    # Tokenize the content and count the number of sentences
    sentences = sent_tokenize(content)
    return len(sentences)


def calculate_page_entropy(content):
    # Tokenize the content
    tokens = word_tokenize(content)

    # Count the occurrences of each token
    token_counter = Counter(tokens)

    # Calculate the probability of each token
    total_tokens = len(tokens)
    token_probabilities = {token: count / total_tokens for token, count in token_counter.items()}

    # Calculate the page entropy using Shannon entropy formula
    entropy = -sum(prob * log2(prob) for prob in token_probabilities.values())

    return entropy


def calculate_number_of_punctuations(content):
    # Count the number of punctuation symbols in the content
    punctuations = [char for char in content if char in string.punctuation]
    return len(punctuations)


def calculate_number_of_capitalizations(content):
    # Count the number of capitalized words in the content
    capitalized_words = re.findall(r"\b[A-Z][a-z]+\b", content)
    return len(capitalized_words)


def calculate_average_number_of_tokens_in_sentence(content):
    # Tokenize the content into sentences
    sentences = sent_tokenize(content)

    # Calculate the total number of tokens in sentences
    total_tokens = sum(len(word_tokenize(sentence)) for sentence in sentences)

    # Calculate the average number of tokens in a sentence
    if len(sentences) !=0:
        average_tokens_in_sentence = total_tokens / len(sentences)
    else:
        average_tokens_in_sentence = 0
    return average_tokens_in_sentence



# Create a list to store the extracted URLs and HTML tag count
data_list = []


# Iterate through each folder in the home directory
for folder_name in os.listdir(home_path):
    Maildir_path = os.path.join(home_path, folder_name,) + '/Maildir/cur/'
    # print(Maildir_path)

    # Store the contents between "Content-Type" and "Content-Transfer-Encoding OR before --_" strings
    content_type_content = ''
    whole_text =''
    label = i = 0
    number_of_script_tags = script_to_body_ratio = 0
    # Iterate through each file in the folder
    for filename in os.listdir(Maildir_path):
        file_path = os.path.join(Maildir_path, filename)
        
        # Check if the current path is a file
        if os.path.isfile(file_path):
            with open(file_path, 'r') as file:
                i += 1
                lines = file.readlines()
                found_content_type = False
                # whole_text = ""
                # Iterate through each line in the file
                for line in lines:
                    # if found_content_type and 'Content-Transfer-Encoding' in line:
                    # if found_content_type and ('--_' or 'Content-Transfer-Encoding') in line:
                    # if found_content_type and '--_' in line:
                    whole_text += line

                    # Extract body of email
                    if found_content_type and '--_' not in line:
                        # Add lines until meet "Content-Transfer-Encoding OR --_"
                        content_type_content += line  # Append the line to the content
                    else:
                        found_content_type = False

                    # Start storing content after finding "Content-Type"
                    if 'Content-Type: text/plain' in line:
                        found_content_type = True  
                
                
    # Print the stored content
    # print(content_type_content)
    # print(f'There are {i} file(s) in this {folder_name} directory.')


    # URL Finder
    print('URL finder:\n')
    body_text = content_type_content

    # Regular expression pattern to match URLs
    url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"

    # Find all URLs in the text
    urls = re.findall(url_pattern, body_text)

    # Calculate HTML length
    html_length = calculate_html_length(whole_text)

    # Calculate number of tokens
    num_tokens = calculate_number_of_tokens(body_text)

    # Calculate number of sentences
    num_sentences = calculate_number_of_sentences(body_text)

    # Calculate page entropy
    page_entropy = calculate_page_entropy(body_text)

    # Calculate number of punctuations
    num_punctuations = calculate_number_of_punctuations(body_text)

    # Calculate number of capitalizations
    num_capitalizations = calculate_number_of_capitalizations(body_text)

    # Calculate average number of tokens in a sentence
    avg_tokens_in_sentence = calculate_average_number_of_tokens_in_sentence(body_text)



    # Append the folder name, URLs, and HTML tag count to the data_list
    data_list.append([folder_name, urls, label, page_entropy ,number_of_script_tags, \
                script_to_body_ratio, html_length, num_tokens,\
                num_sentences, num_punctuations, num_capitalizations, \
                avg_tokens_in_sentence, count_html_tags(whole_text) \
                  ])

    # Print the found URLs
    for url in urls:
        print(url)

    # Count HTML tags in the content
    html_tag_count = count_html_tags(whole_text)
    print("Number of HTML tags:", html_tag_count)

    # print ('********** EOF **********')

# Save the data in a CSV file
csv_file_path = './Email CentOS/extract_feature.csv'
with open(csv_file_path, 'a', newline='') as csv_file:
    csv_writer = csv.writer(csv_file)
    # Write the header
    csv_writer.writerow(['username', 'url', 'label', 'page_entropy', 'number_of_script_tags', \
            'script_to_body_ratio' , 'length_of_html','number_of_tokens', \
            'number_of_sentences', 'number_of_punctuations','number_of_capitalizations' \
            , 'average_number_of_tokens_in_sentence', 'number_of_html_tags' \
            ])
    # Write the data
    for row in data_list:
        csv_writer.writerow(row)


print("Data saved to", csv_file_path)


