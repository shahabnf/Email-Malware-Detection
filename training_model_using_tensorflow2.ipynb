{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b2db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data (replace the filename with your actual dataset)\n",
    "data = pd.read_csv('feature-updated-dataset.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X_textual = data['url']  # Textual data (URLs)\n",
    "# X_numerical = data.drop(columns=['url', 'label'])  # Numerical data excluding URLs and labels\n",
    "X_numerical = data.iloc[:, 2:12]  # Extract first 10 numerical features (columns 2 to 11)\n",
    "\n",
    "y = data['label']\n",
    "\n",
    "# Define the number of folds (k) for cross-validation\n",
    "k = 5\n",
    "\n",
    "# Initialize the Tokenizer for textual data\n",
    "max_words = 5000  # The maximum number of words to keep in the vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_textual)\n",
    "\n",
    "# Convert textual data to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(X_textual)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "X_textual = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Define the number of classes (malicious and not malicious)\n",
    "num_classes = 2\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "# Initialize arrays to store cross-validation results\n",
    "accuracy_scores = []\n",
    "\n",
    "# Create the k-fold cross-validation object\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X_textual):\n",
    "    X_textual_train, X_textual_test = X_textual[train_index], X_textual[test_index]\n",
    "    X_numerical_train, X_numerical_test = X_numerical[train_index], X_numerical[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Create the LSTM model for textual data\n",
    "    input_textual = Input(shape=(max_sequence_length,))\n",
    "    x = Embedding(input_dim=max_words, output_dim=32, input_length=max_sequence_length)(input_textual)\n",
    "    x = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "    textual_output = Dense(32)(x)\n",
    "\n",
    "    # Create the Dense model for numerical data\n",
    "    input_numerical = Input(shape=(X_numerical_train.shape[1],))\n",
    "    numerical_output = Dense(32)(input_numerical)\n",
    "\n",
    "    # Concatenate the textual and numerical outputs\n",
    "    concatenated = concatenate([textual_output, numerical_output])\n",
    "    output = Dense(num_classes, activation='softmax')(concatenated)\n",
    "\n",
    "    # Create the combined model\n",
    "    model = Model(inputs=[input_textual, input_numerical], outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit([X_textual_train, X_numerical_train], y_train, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    _, accuracy = model.evaluate([X_textual_test, X_numerical_test], y_test)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_model_GPT.joblib')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save('trained_model_GPT.joblib')\n",
    "# print(\"Model saved successfully!\")\n",
    "\n",
    "# Save the fitted scaler to a file\n",
    "# joblib.dump(scaler, 'fitted_scaler.pkl')\n",
    "\n",
    "import joblib\n",
    "joblib.dump(model, 'trained_model_GPT2.joblib', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer using joblib\n",
    "tokenizer_filename = 'tokenizer.joblib'\n",
    "tokenizer = joblib.load(tokenizer_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c6008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import joblib\n",
    "# import \n",
    "\n",
    "\n",
    "# Function to preprocess textual data and numerical features\n",
    "def preprocess_data(url, numerical_features, tokenizer, fitted_scaler):\n",
    "    # Preprocess textual data (URL)\n",
    "    sequences = tokenizer.texts_to_sequences([url])\n",
    "    max_sequence_length = 86 # Insert the value used during training\n",
    "    X_textual = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Preprocess numerical data\n",
    "    numerical_features = fitted_scaler.transform([numerical_features])\n",
    "\n",
    "    return X_textual, numerical_features\n",
    "\n",
    "# Function to check if the URL is malicious\n",
    "def is_malicious(url, numerical_features, tokenizer, fitted_scaler, model):\n",
    "    X_textual, X_numerical = preprocess_data(url, numerical_features, tokenizer, fitted_scaler)\n",
    "    y_pred = model.predict([X_textual, X_numerical])\n",
    "    is_malicious = np.argmax(y_pred, axis=1)[0]  # Get the class prediction (0 or 1)\n",
    "\n",
    "    # Assuming class 0 represents non-malicious and class 1 represents malicious\n",
    "    if is_malicious == 0:\n",
    "        return \"Non-Malicious\"\n",
    "    else:\n",
    "        return \"Malicious\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model\n",
    "#     model = load_model('trained_model_GPT3.joblib')\n",
    "    model_filename = 'trained_model_GPT3.joblib'\n",
    "    model = joblib.load(model_filename)\n",
    "\n",
    "    # Load the Tokenizer and fitted scaler used during training\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts([''])  # Empty list to initialize the tokenizer (URLs will be added later)\n",
    "    fitted_scaler = joblib.load('fitted_scaler.pkl')\n",
    "\n",
    "    # Example usage\n",
    "#     url_input = \"https://google.com\"  # Replace with the URL to be checked\n",
    "#     url_input = \"http://162.144.84.82/~tdsecureupdate/Tdbank.com/step2.php\"\n",
    "#     url_input = \"https://yolo-mkt.cl/x/att/index.html\"\n",
    "#     numerical_features_input = [0,0,0,0,0,0,0,0,0,0]\n",
    "    # url_input = \"http://162.144.84.82/~tdsecureupdate/Tdbank.com/login.php\"\n",
    "    # numerical_features_input = [4.847764373,0,0,10133,427,60,640,1766,7.5,30]\n",
    "#     numerical_features_input = [4.5, 0.5, 0.3, 0.2, 0.7, 0.4, 0.9, 0.6, 0.8, 0.2]\n",
    "    # url_input = \"http://leverdistribuidora.com.br/images/customer/online/homepage.htm\"\n",
    "    # numerical_features_input = [1,4.625598281,0,0,315,41,4,8,43,11]\n",
    "    # url_input = \"http://soilex.co.ke/amisc/login/index.php?email=abuse@fsicencal.com\"\n",
    "    # numerical_features_input = [1,5.19399201,91,0.174376893,107325,5602,2158,16231,1523,3.33781279]\n",
    "    # url_input = \"http://forum.skyscraperpage.com/showthread.php?t=190904&goto=newpost\"\n",
    "    # numerical_features_input = [0,5.266849978,36,0.034339963,171695,15371,1584,20679,3053,10.52525253]\n",
    "    # http://itunes.apple.com/us/artist/john-kaizan-neptune/id2726341,0,5.209235181,11,0.513672308,392143,17778,13123,62097,26610,2.323782672,963\n",
    "    # url_input = 'http://itunes.apple.com/us/artist/john-kaizan-neptune/id2726341'\n",
    "    # numerical_features_input = [5.209235181,11,0.513672308,392143,17778,13123,62097,26610,2.323782672,963]\n",
    "    # url_input = 'http://forum.skyscraperpage.com/showthread.php?t=190904&goto=newpost'\"training_model_using_lstm_ongoing - Shahab GPT - Model Works.ipynb\"\n",
    "    # numerical_features_input = [5.266849978,36,0.034339963,171695,15371,1584,20679,3053,10.52525253,2439]\n",
    "    # numerical_features_input = [5.3,5.266849978,36,0.034339963,171695,15371,1584,20679,3053,10.52525253]\n",
    "    # url_input = \"http://forums.canadian-tv.com/showthread.php?27329-HDTV-Channels-Directory-Canada-amp-US\"\n",
    "    # numerical_features_input = [4.623120867,0,0,196,23,3,6,38,8.333333333,6]\n",
    "    # url_input = \"http://forums.canadian-tv.com/showthread.php?27329-HDTV-Channels-Directory-Canada-amp-US\"\n",
    "    # numerical_features_input = [4.623120867,0,0,196,23,3,6,38,8.333333333,6]\n",
    "    # url_input = \"http://resvis.com\"\n",
    "    url_input = \"http://apple.com/\"\n",
    "    numerical_features_input =[5,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    # Check if the URL is malicious\n",
    "    result = is_malicious(url_input, numerical_features_input, tokenizer, fitted_scaler, model)\n",
    "    print(f\"The URL is {result}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733bc57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "# A helper function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "   \n",
    "\n",
    "def extract_content_features(url):\n",
    "    # Preprocess the URL text before extracting features\n",
    "    url_text = preprocess_text(url)\n",
    "    \n",
    "    # Count the number of script tags\n",
    "    number_of_script_tags = len(re.findall(r'<script', url, flags=re.IGNORECASE))\n",
    "\n",
    "    # Calculate script-to-body ratio\n",
    "    script_tag_length = len(\"\".join(re.findall(r'<script.*?</script>', url, flags=re.IGNORECASE)))\n",
    "    body_length = len(\"\".join(re.findall(r'<body.*?</body>', url, flags=re.IGNORECASE)))\n",
    "    script_to_body_ratio = script_tag_length / (body_length + 1e-10)\n",
    "\n",
    "    # Calculate the length of HTML\n",
    "    length_of_html = len(url)\n",
    "\n",
    "    # Tokenize the URL and calculate the number of tokens\n",
    "    tokens = url.split()\n",
    "    number_of_tokens = len(tokens)\n",
    "\n",
    "    # Calculate the number of sentences (assumes URLs are separated by dots)\n",
    "    number_of_sentences = len(url.split('.'))\n",
    "\n",
    "    # Calculate the number of punctuations\n",
    "    number_of_punctuations = len(re.findall(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', url))\n",
    "\n",
    "    # Calculate the number of capitalizations\n",
    "    number_of_capitalizations = len(re.findall(r'[A-Z]', url))\n",
    "\n",
    "    # Calculate average number of tokens in a sentence\n",
    "    average_number_of_tokens_in_sentence = number_of_tokens / (number_of_sentences + 1e-10)\n",
    "\n",
    "    # Calculate the number of HTML tags\n",
    "    number_of_html_tags = len(re.findall(r'<[^>]+>', url))\n",
    "\n",
    "    return 3, number_of_script_tags, script_to_body_ratio, length_of_html, \\\n",
    "           number_of_tokens, number_of_sentences, number_of_punctuations, \\\n",
    "           number_of_capitalizations, average_number_of_tokens_in_sentence, number_of_html_tags\n",
    "\n",
    "# url = \"http://www.example.com\"\n",
    "# url_input = \"https://yahoo.com\"  # Replace with the URL to be checked\n",
    "\n",
    "# extract_content_features(url_input)\n",
    "\n",
    "\n",
    "# Function to preprocess textual data and numerical features\n",
    "def preprocess_data(url, numerical_features, tokenizer, fitted_scaler):\n",
    "    # Preprocess textual data (URL)\n",
    "    sequences = tokenizer.texts_to_sequences([url])\n",
    "    max_sequence_length = 86 # Insert the value used during training\n",
    "    X_textual = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Preprocess numerical data\n",
    "    numerical_features = fitted_scaler.transform([numerical_features])\n",
    "\n",
    "    return X_textual, numerical_features\n",
    "\n",
    "# Function to check if the URL is malicious\n",
    "def is_malicious(url, numerical_features, tokenizer, fitted_scaler, model):\n",
    "    X_textual, X_numerical = preprocess_data(url, numerical_features, tokenizer, fitted_scaler)\n",
    "    y_pred = model.predict([X_textual, X_numerical])\n",
    "    is_malicious = np.argmax(y_pred, axis=1)[0]  # Get the class prediction (0 or 1)\n",
    "\n",
    "    # Assuming class 0 represents non-malicious and class 1 represents malicious\n",
    "    if is_malicious == 0:\n",
    "        return \"Non-Malicious\"\n",
    "    else:\n",
    "        return \"Malicious\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model\n",
    "#     model = load_model('trained_model_GPT3.joblib')\n",
    "    model_filename = 'trained_model_GPT3.joblib'\n",
    "    model = joblib.load(model_filename)\n",
    "\n",
    "    # Load the Tokenizer and fitted scaler used during training\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts([''])  # Empty list to initialize the tokenizer (URLs will be added later)\n",
    "    fitted_scaler = joblib.load('fitted_scaler.pkl')\n",
    "\n",
    "    # Example usage\n",
    "    url_input = \"https://google.com\"  # Replace with the URL to be checked\n",
    "#     url_input = \"http://162.144.84.82/~tdsecureupdate/Tdbank.com/step2.php\"\n",
    "#     url_input = \"https://yolo-mkt.cl/x/att/index.html\"\n",
    "#     url_input = \"http://depradarn.ml/2/share/file.html\"\n",
    "#     url_input = \"http://verifcomptewsyup.com/auth\"\n",
    "#     url_input = \"https://bit.ly/2HL4lmh\"\n",
    "#     numerical_features_input = [0.1, 0.5, 0.3, 0.2, 0.7, 0.4, 0.9, 0.6, 0.8, 0.2]\n",
    "    numerical_features_input = extract_content_features(url_input)\n",
    "    print(numerical_features_input)\n",
    "    # Check if the URL is malicious\n",
    "    result = is_malicious(url_input, numerical_features_input, tokenizer, fitted_scaler, model)\n",
    "    print(f\"The URL is {result}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e531ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import joblib\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "def calculate_page_entropy(content):\n",
    "    # Tokenize the content\n",
    "    tokens = word_tokenize(content)\n",
    "\n",
    "    # Count the occurrences of each token\n",
    "    token_counter = Counter(tokens)\n",
    "\n",
    "    # Calculate the probability of each token\n",
    "    total_tokens = len(tokens)\n",
    "    token_probabilities = {token: count / total_tokens for token, count in token_counter.items()}\n",
    "\n",
    "    # Calculate the page entropy using Shannon entropy formula\n",
    "    entropy = -sum(prob * log2(prob) for prob in token_probabilities.values())\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# A helper function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "   \n",
    "\n",
    "def extract_content_features(url):\n",
    "    # Preprocess the URL text before extracting features\n",
    "    url_text = preprocess_text(url)\n",
    "    \n",
    "#     calculate_page_entropy\n",
    "    # Calculate page entropy\n",
    "    page_entropy = calculate_page_entropy(url_text)\n",
    "    \n",
    "    # Count the number of script tags\n",
    "    number_of_script_tags = len(re.findall(r'<script', url, flags=re.IGNORECASE))\n",
    "\n",
    "    # Calculate script-to-body ratio\n",
    "    script_tag_length = len(\"\".join(re.findall(r'<script.*?</script>', url, flags=re.IGNORECASE)))\n",
    "    body_length = len(\"\".join(re.findall(r'<body.*?</body>', url, flags=re.IGNORECASE)))\n",
    "    script_to_body_ratio = script_tag_length / (body_length + 1e-10)\n",
    "\n",
    "    # Calculate the length of HTML\n",
    "    length_of_html = len(url)\n",
    "\n",
    "    # Tokenize the URL and calculate the number of tokens\n",
    "    tokens = url.split()\n",
    "    number_of_tokens = len(tokens)\n",
    "\n",
    "    # Calculate the number of sentences (assumes URLs are separated by dots)\n",
    "    number_of_sentences = len(url.split('.'))\n",
    "\n",
    "    # Calculate the number of punctuations\n",
    "    number_of_punctuations = len(re.findall(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', url))\n",
    "\n",
    "    # Calculate the number of capitalizations\n",
    "    number_of_capitalizations = len(re.findall(r'[A-Z]', url))\n",
    "\n",
    "    # Calculate average number of tokens in a sentence\n",
    "    average_number_of_tokens_in_sentence = number_of_tokens / (number_of_sentences + 1e-10)\n",
    "\n",
    "    # Calculate the number of HTML tags\n",
    "    number_of_html_tags = len(re.findall(r'<[^>]+>', url))\n",
    "\n",
    "    return page_entropy, number_of_script_tags, script_to_body_ratio, length_of_html, \\\n",
    "           number_of_tokens, number_of_sentences, number_of_punctuations, \\\n",
    "           number_of_capitalizations, average_number_of_tokens_in_sentence, number_of_html_tags\n",
    "\n",
    "# url = \"http://www.example.com\"\n",
    "# url_input = \"https://yahoo.com\"  # Replace with the URL to be checked\n",
    "\n",
    "# extract_content_features(url_input)\n",
    "\n",
    "\n",
    "# Function to preprocess textual data and numerical features\n",
    "def preprocess_data(url, numerical_features, tokenizer, fitted_scaler):\n",
    "    # Preprocess textual data (URL)\n",
    "    sequences = tokenizer.texts_to_sequences([url])\n",
    "    max_sequence_length = 86 # Insert the value used during training\n",
    "    X_textual = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Preprocess numerical data\n",
    "    numerical_features = fitted_scaler.transform([numerical_features])\n",
    "\n",
    "    return X_textual, numerical_features\n",
    "\n",
    "# Function to check if the URL is malicious\n",
    "def is_malicious(url, numerical_features, tokenizer, fitted_scaler, model):\n",
    "    X_textual, X_numerical = preprocess_data(url, numerical_features, tokenizer, fitted_scaler)\n",
    "    y_pred = model.predict([X_textual, X_numerical])\n",
    "    is_malicious = np.argmax(y_pred, axis=1)[0]  # Get the class prediction (0 or 1)\n",
    "\n",
    "    # Assuming class 0 represents non-malicious and class 1 represents malicious\n",
    "    if is_malicious == 0:\n",
    "        return \"Non-Malicious\"\n",
    "    else:\n",
    "        return \"Malicious\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model\n",
    "#     model = load_model('trained_model_GPT3.joblib')\n",
    "    model_filename = 'trained_model_GPT3.joblib'\n",
    "    model = joblib.load(model_filename)\n",
    "\n",
    "    # Load the Tokenizer and fitted scaler used during training\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts([''])  # Empty list to initialize the tokenizer (URLs will be added later)\n",
    "    fitted_scaler = joblib.load('fitted_scaler.pkl')\n",
    "\n",
    "    # Example usage\n",
    "    # url_input = \"https://google.com\"  # Replace with the URL to be checked\n",
    "    # url_input = \"http://162.144.84.82/~tdsecureupdate/Tdbank.com/step2.php\"\n",
    "#     url_input = \"https://yolo-mkt.cl/x/att/index.html\"\n",
    "#     url_input = \"http://depradarn.ml/2/share/file.html\"\n",
    "#     url_input = \"http://verifcomptewsyup.com/auth\"\n",
    "#     numerical_features_input = [0.1, 0.5, 0.3, 0.2, 0.7, 0.4, 0.9, 0.6, 0.8, 0.2]\n",
    "    numerical_features_input = extract_content_features(url_input)\n",
    "    print(numerical_features_input)\n",
    "    # Check if the URL is malicious\n",
    "    result = is_malicious(url_input, numerical_features_input, tokenizer, fitted_scaler, model)\n",
    "    print(f\"The URL is {result}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
